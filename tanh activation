import numpy as np

class DenseLayer:
    def __init__(self, input_size, output_size):
        self.weights = np.random.randn(input_size, output_size)
        self.biases = np.zeros((1, output_size))
        self.inputs = None
        self.outputs = None
        self.dweights = None
        self.dbiases = None

    def forward(self, inputs):
        self.inputs = inputs
        self.outputs = np.dot(inputs, self.weights) + self.biases
        return self.outputs

    def backward(self, dvalues):
        self.dweights = np.dot(self.inputs.T, dvalues)
        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)
        return np.dot(dvalues, self.weights.T)

class TanhActivation:
    def forward(self, inputs):
        self.inputs = inputs
        return np.tanh(inputs)

    def backward(self, dvalues):
        return dvalues * (1 - np.tanh(self.inputs) ** 2)

class NeuralNetwork:
    def __init__(self):
        self.layers = []

    def add_layer(self, layer):
        self.layers.append(layer)

    def forward(self, inputs):
        for layer in self.layers:
            inputs = layer.forward(inputs)
        return inputs

    def backward(self, dvalues):
        for layer in reversed(self.layers):
            dvalues = layer.backward(dvalues)
        return dvalues

nn_tanh = NeuralNetwork()

nn_tanh.add_layer(DenseLayer(2, 4))
nn_tanh.add_layer(TanhActivation())
nn_tanh.add_layer(DenseLayer(4, 1))

x = np.array([[0.1, 0.2]])
output_tanh = nn_tanh.forward(x)
print("Output with Tanh Activation:", output_tanh)
dummy_gradients = np.array([[0.1]])
dvalues_tanh = nn_tanh.backward(dummy_gradients)
print("Gradients after backward pass with Tanh Activation:", dvalues_tanh)
